{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3: Graphical Summaries and Intro to Data Wrangling \n",
    "***\n",
    "\n",
    "In this notebook you'll apply some basic Pandas tools to perform graphical summaries on the Boulder Weather Data from Lecture 2.  Then we'll look at a dirty version of the Titanic data set and see if we can wrangle it into submission. \n",
    "\n",
    "First, as always, we'll load Numpy and Pandas using their common aliases, np and pd. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to load Matplotlib's Pylab library and to set up Jupyter so that it will plot directly in the notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the weather data into a Pandas DataFrame using read_csv( ).  Remember to change the file_path variable to point to the correct location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Two different paths to the data \n",
    "local_path = 'data/clean_boulder_weather.csv'\n",
    "web_path   = 'https://raw.githubusercontent.com/chrisketelsen/csci3022/master/inclass-notebooks/data/clean_boulder_weather.csv'\n",
    "\n",
    "# Select the path that works for you \n",
    "file_path = web_path \n",
    "\n",
    "# Load the data into a DataFrame \n",
    "dfW= pd.read_csv(file_path)\n",
    "\n",
    "# Inspect some of the data\n",
    "dfW.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the data has the following columns: \n",
    "\n",
    "- **STATION**: The unique identification code for each weather station \n",
    "- **NAME**: The location / name of the weather station \n",
    "- **DATE**: The date of the observation \n",
    "- **PRCP**: The precipitation (in inches)\n",
    "- **TMAX**: The daily maximum temperature (in Fahrenheit)\n",
    "- **TMIN**: The daily minimum temperature (in Fahrenheit)\n",
    "\n",
    "And recall that the data set contains data from several weather stations.  Their names were"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "station_names = list(set(dfW[\"NAME\"]))\n",
    "print(\"Station Names: \", station_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms of Maximum Temperature \n",
    "\n",
    "Suppose we want to draw a histogram of the **TMAX** characteristics over all weather stations.  There are several ways to plot histograms in Python.  We will use Pandas built-in histogram function because it is designed to handle missing data well, but Matplotlib has a [native hist( )](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.hist.html) function that you can use if you prefer. Furthermore, the Pandas' hist( ) function actually calls the Matplotlib hist( ) function internally, so any parameters that work with Matplotlib's function should work with the Pandas function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "# Plot histogram \n",
    "dfW.hist(column=\"TMAX\", ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've made a bare-bones histogram of the **TMAX** data using Pandas' hist( ) function.  Notice that we call the hist( ) function on the entire DataFrame and then indicate which column we want to use using the $\\texttt{column}$ option.  Finally, we pass in the figure axis to the function so that we can make modifications to the layout later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas' automatic bin selection is usually pretty good, but if you want to define your own bins you can do so by passing in an optional $\\texttt{bins}$ argument to hist( ).  The two options are to pass in an integer value for the number of bins you want _or_ a list specifying the bin edges. Suppose for example we want bins of width 5 starting at 50 and ending at 105.  We could do the following:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_bins = range(50,110,5)\n",
    "print(\"bin edges = \", list(my_bins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize figure \n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "# Plot histogram \n",
    "dfW.hist(column=\"TMAX\", ax=ax, bins=my_bins);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add loads of options to make the plot more appealing to the eye.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize figure \n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "# Plot histogram with custom colors\n",
    "dfW.hist(column=\"TMAX\", ax=ax, bins=my_bins, facecolor=\"steelblue\", edgecolor=\"white\")\n",
    "\n",
    "# Add a title\n",
    "ax.set_title(\"Boulder County Max Temperatures\", fontsize=20)\n",
    "\n",
    "# Add axis labels \n",
    "ax.set_xlabel(\"Max Temperature (in Fahrenheit)\", fontsize=16)\n",
    "ax.set_ylabel(\"Frequency\", fontsize=16)\n",
    "\n",
    "# Make the grid lines lighter and put them behind data \n",
    "ax.grid(alpha=0.25)\n",
    "ax.set_axisbelow(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, side-by-side or stacked histograms are a great way to compare two sets of data.  Let's create histograms of **TMAX** for data from two different weather stations and stack them on top of each other.  Note that in order to make a true comparison it's a good idea to set the range on the horizontal and vertical axes to be the same on both plots. For this experiment we'll use the data from the Niwot and Sugarloaf weather stations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize figure subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8,8))\n",
    "\n",
    "# --------------------------------------\n",
    "# Plot histogram for Niwot on Top \n",
    "# --------------------------------------\n",
    "dfW.loc[dfW[\"NAME\"]==\"NIWOT, CO US\"].hist(column=\"TMAX\", ax=axes[0], facecolor=\"steelblue\", edgecolor=\"white\")\n",
    "                                                      \n",
    "# Add titles and labels \n",
    "axes[0].set_title(\"Niwot Max Daily Temperatures\", fontsize=20)\n",
    "axes[0].set_xlabel(\"Max Temperature (in degrees F)\", fontsize=16)\n",
    "axes[0].set_ylabel(\"Frequency\", fontsize=16)\n",
    "\n",
    "# Make grid lighter and set behind data\n",
    "axes[0].grid(alpha=0.25)\n",
    "axes[0].set_axisbelow(True)\n",
    "\n",
    "# --------------------------------------\n",
    "# Plot histogram for Sugarloaf on Bottom \n",
    "# --------------------------------------\n",
    "dfW.loc[dfW[\"NAME\"]==\"SUGARLOAF COLORADO, CO US\"].hist(column=\"TMAX\", ax=axes[1], facecolor=\"green\", edgecolor=\"white\")\n",
    "                                                      \n",
    "# Add titles and labels \n",
    "axes[1].set_title(\"Sugarloaf Max Daily Temperatures\", fontsize=20)\n",
    "axes[1].set_xlabel(\"Max Temperature (in degrees F)\", fontsize=16)\n",
    "axes[1].set_ylabel(\"Frequency\", fontsize=16)\n",
    "\n",
    "# Make grid lighter and set behind data\n",
    "axes[1].grid(alpha=0.25)\n",
    "axes[1].set_axisbelow(True)\n",
    "\n",
    "# --------------------------------------\n",
    "# Make the plots comparable \n",
    "# --------------------------------------\n",
    "\n",
    "# Set x and y axis limits to match\n",
    "axes[0].set_xlim([50,95])\n",
    "axes[1].set_xlim([50,95])\n",
    "axes[0].set_ylim([0,9])\n",
    "axes[1].set_ylim([0,9]);\n",
    "\n",
    "# Adjust vertical space so titles/axis labels don't overlap \n",
    "fig.subplots_adjust(hspace=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because we set the horizontal and vertical axis ranges to be the same we can easily make visual comparisons between the data.  For instance it is crystal clear that that it tends to be hotter in general in Sugarloaf than in Niwot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "***\n",
    "Make stacked histograms to compare the minimum temperatures at Ralston Resevoir and Gross Resevoir.  Play around with the different parameters until you find a style and color scheme that you like.  Don't forget to add meaningful titles and axis labels! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've gotten some decent _Frequency_ histograms, pass the parameter **normed=True** into the hist( ) function and redo the plots.  Note that you'll probably have to change the vertical axis limits.  How does the **normed=True** parameter change the histograms? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2  \n",
    "***\n",
    "Make stacked histograms to compare the maximum temperature over all of Boulder County on days when it rained vs days when it did not rain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box-and-Whisker Plots of Minimum Temperature \n",
    "***\n",
    "\n",
    "Next we'll look at Pandas' box-and-whisker plot functionality using the boxplot( ) function.  We'll start by making a box-and-whisker plot for Niwot's minimum daily temperatures. Like the hist( ) function, boxplot( ) is called on the entire DataFrame, and then we specify which column we're interested in using the **column** parameter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize figure \n",
    "fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "# Gat axis object \n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Plot histogram \n",
    "dfW.loc[dfW[\"NAME\"]=='GROSS RESERVOIR, CO US'].boxplot(column=\"TMIN\", ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result is a standard box-and-whiskers plot with a box spanning the distance between the lower and upper quartiles, a (barely perceptible) median line, whiskers and caps, and outliers that fall more than $1.5 \\times IQR$ outside of the nearest quartile. \n",
    "\n",
    "Unfortunately, the default parameters don't lend themselves well to easy-to-read plots, so we'll make a few tweaks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize figure \n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "\n",
    "# Plot histogram, but this time return dictionary of style parameters for modification after the fact \n",
    "bp = dfW.loc[dfW[\"NAME\"]=='GROSS RESERVOIR, CO US'].boxplot(column=\"TMIN\", ax=ax, widths=[.25], return_type='dict');\n",
    "\n",
    "# ---------------------------------------\n",
    "# Set properties of various parts of plot \n",
    "# ---------------------------------------\n",
    "\n",
    "# Change properties of boxes \n",
    "for box in bp['boxes']:\n",
    "    box.set(color='steelblue', linewidth=2)\n",
    "    \n",
    "# Change properties of whiskers \n",
    "for whisker in bp['whiskers']:\n",
    "    whisker.set(color='gray', linewidth=2)\n",
    "    \n",
    "# Change properties of caps \n",
    "for cap in bp['caps']:\n",
    "    cap.set(color='gray', linewidth=2)\n",
    "    \n",
    "# Change properties of median \n",
    "for cap in bp['medians']:\n",
    "    cap.set(color='green', linewidth=2, alpha=0.5)\n",
    "    \n",
    "# Change properties of fliers (outliers) \n",
    "for flier in bp['fliers']:\n",
    "    flier.set(markerfacecolor='steelblue', linewidth=2, marker='s', markersize=6, alpha=0.5)\n",
    "\n",
    "# Set title and vertical axis label\n",
    "ax.set_title('GROSS RESERVOIR, CO US', fontsize=18)\n",
    "ax.set_ylabel(\"Min Temperature (in degrees F)\", fontsize=16)\n",
    "\n",
    "# Make grid-lines lighter\n",
    "ax.grid(alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the default settings for boxplot( ) gives us whiskers and outliers as described in lecture.  That is, the length of the whiskers are set equal to the largest distance between an observation and the upper or lower quartile that is less than $1.5 \\times IQR$.  Any data point beyond this distance is drawn as an outlier (or a _flier_ as Pandas calls them).   \n",
    "\n",
    "Box-and-whisker plots are great for visualizing the spread of the data, outliers, and skew of the data.  For instance, from this box-and-whisker plot we can tell that the minimum temperature at Gross Reservoir has a positive skew because the the upper quartile $Q_3$ is farther from the median than the lower quartile $Q_1$ is from the median. \n",
    "\n",
    "Box-and-whisker plots are especially useful for comparing multiple sets of observations.  For instance, the following side-by-side box-and-whisker plots compare the minimum temperature at Gross Reservoir and Ralston Reservoir. \n",
    "\n",
    "Note that the trick here is to select the rows of the DataFrame corresponding only to Gross Reservoir and Ralston Reservoir and then call boxplot( ) with the **by** parameter to break the data into the two sets based on the value of the **NAME** column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize figure \n",
    "fig, ax = plt.subplots(figsize=(6,8))\n",
    "\n",
    "# To get just the columns corresponding to Gross and Ralston reservoirs, we \n",
    "# select only the rows of the DataFrame coresponding to those names, then \n",
    "# call boxplot and pass by=\"NAME\" to break the data into the two sets \n",
    "bp = dfW.loc[(dfW[\"NAME\"]=='GROSS RESERVOIR, CO US') | (dfW[\"NAME\"]=='RALSTON RESERVOIR, CO US')].boxplot(\n",
    "    column=[\"TMIN\"], by=\"NAME\", ax=ax, widths=[.5, .5], return_type=\"dict\")\n",
    "\n",
    "# This time, bp is a Pandas series containing multiple dictionaries\n",
    "# corresponding to the two sets of data. We loop over each one individually \n",
    "# and set the parameters we want \n",
    "for column in bp:\n",
    "    \n",
    "    # Change properties in boxes \n",
    "    for box in column['boxes']:\n",
    "        box.set(color='steelblue', linewidth=2)\n",
    "    \n",
    "    # Change properties of whiskers \n",
    "    for whisker in column['whiskers']:\n",
    "        whisker.set(color='gray', linewidth=2)\n",
    "\n",
    "    # Change properties of caps \n",
    "    for cap in column['caps']:\n",
    "        cap.set(color='gray', linewidth=2)\n",
    "\n",
    "    # Change properties of median \n",
    "    for cap in column['medians']:\n",
    "        cap.set(color='green', linewidth=2, alpha=0.5)\n",
    "\n",
    "    # Change properties of fliers (outliers) \n",
    "    for cap in column['fliers']:\n",
    "        cap.set(markerfacecolor='steelblue', linewidth=2, marker='s', markersize=6, alpha=0.5)\n",
    "\n",
    "# Set title and vertical axis label\n",
    "ax.set_title('GROSS vs. RALSTON RESERVOIRS', fontsize=18)\n",
    "ax.set_ylabel(\"Min Temperature (in F)\", fontsize=16)\n",
    "\n",
    "# Set names of plots\n",
    "plt.xticks([1,2],[\"Gross\", \"Ralston\"], rotation=0, fontsize=16)\n",
    "\n",
    "# Get rid of automatically generated titles and xlables\n",
    "plt.suptitle(\"\")\n",
    "ax.set_xlabel(\"\")\n",
    "\n",
    "# Make grid-lines lighter\n",
    "ax.grid(alpha=0.25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the side-by-side box-and-whiskers plots we can easily see several things \n",
    "\n",
    "- the median min daily temperature at Ralston Reservoir is more than 10 degrees warmer than that at Gross Reservoir\n",
    "- the min daily temperature at Gross Res is positively skewed, while the min daily temperature at Ralston Res is slightly negatively skewed \n",
    "- while the overall spreads of the two data sets are approximately equal, there is more variability in the middle 50% of the min daily temp at Gross Res than in the middle 50% of the min daily temp at Ralston Res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "***\n",
    "\n",
    "Plot side-by-side box and whisker plots of the max daily temperature in Niwot and Sugarloaf.  Comment on any similarities and differences that you see in the distributions of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "***\n",
    "\n",
    "Plot side-by-side box and whisker plots of the precipitation measured at **all** weather stations where we have precipitation data (**hint**: it's not all of them).  Mess with the plot parameters until you get a reasonable graphic that doesn't look too cramped.  Note that it might be helpful to make the figure much wider and rotate the horizontal labels 90 degrees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangling and Cleaning Data in Pandas\n",
    "***\n",
    "\n",
    "In this section we will explore some common Pandas functionality for cleaning and wrangling data.  We will explore a dirtied-up version of the Titanic data. First, load the data into a Pandas DataFrame called dfDirtyT using read_csv( ).  Remember to change the file_path variable to point to the correct location.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Two different paths to the data \n",
    "local_path = 'data/dirty_titanic_data.csv'\n",
    "web_path   = 'https://raw.githubusercontent.com/chrisketelsen/csci3022/master/inclass-notebooks/data/dirty_titanic_data.csv'\n",
    "\n",
    "# Select the path that works for you \n",
    "file_path = web_path\n",
    "\n",
    "# Load the data into a DataFrame \n",
    "dfDirtyT = pd.read_csv(file_path)\n",
    "\n",
    "# Inspect some of the data\n",
    "dfDirtyT.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that each row in the DataFrame refers to a particular passenger on the Titanic.  The columns of the DataFrame give you specific information about each passenger.  The **PassengerId** is simply a unique identifier given to each passenger in the data set.  The rest of the attributes are more meaningful: \n",
    "\n",
    "- **Survived**: Indicates whether the passenger survived the sinking\n",
    "- **Pclass**: Indicates the socio-economic status of the passenger (lower number means higher class)\n",
    "- **Name**: The passenger's name \n",
    "- **Sex**: The passenger's sex \n",
    "- **Age**: The passenger's age\n",
    "- **SibSp**: The number of siblings / spouses the passenger was traveling with \n",
    "- **Parch**: The number of children / parents the passenger was traveling with \n",
    "- **Ticket**: The passenger's ticket number \n",
    "- **Fare**: How much the passenger paid for their ticket \n",
    "- **Embarked**: The passenger's port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, first let's get the lay of the land.  It's almost guaranteed that in any real-world data set you're going to have some values that are simply missing.  Another common occurrence is that values that really should be of numerical type are loaded into Pandas as strings instead of ints or floats.  One way to check all of these things at once is using Pandas info( ) function on your DataFrame.  Let's try that now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfDirtyT.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that there are 891 total rows in the DataFrame.  Furthermore, the info( ) function gives us a report about how many non-missing (non-null) values we have in each column.  For instance, it appears that each row in the DataFrame contains a valid **Name** entry, but only 861 rows have a valid **Survived** entry.  Other columns that are missing data are **Pclass**, **Age**, **Cabin** and **Embarked**. \n",
    "\n",
    "Finally, the last column of the report from info( ) tells us the data type of the non-missing entries.  Note that, as expected, **SibSp** and **Parch** are recorded as integers.  If a data type is reported as _object_ then this typically indicates a _string_ in Pandas.  \n",
    "\n",
    "Do any of the columns have data types that are contrary to what you would expect? \n",
    "\n",
    "How about **Age** and **Fare**? We would expect these to be ints or floats, but Pandas is telling us that they are strings.  A closer inspection of the call to head( ) above shows that some knucklehead has formatted **Age** as a string with a number and the abbreviation \"yrs\".  Furthermore, the **Fare** is given as a string reporting the cost of the ticket in English pounds and shillings. If we want to compute summary statistics on these columns we're going to have to convert them to numerical types.  \n",
    "\n",
    "But first, let's take care of the missing data.  Notice that **Cabin** is missing so many values that the entire column is unlikely to be useful.  We can delete an entire column of a DataFrame in place as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del dfDirtyT[\"Cabin\"]\n",
    "dfDirtyT.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the new call to info( ) that the **Cabin** column is no more.  \n",
    "\n",
    "We're still missing values in **Survived**, **Pclass**, **Age**, and **Embarked**.  If we have enough data, we might consider deleting every row in the DataFrame that contains any missing values.  We can do so with the .dropna( ) function. But in this case, we have plans for the missing values in the **Age** column.  To drop all rows with missing values in a _subset_ of the columns, we can pass the **subset** parameter to .dropna( ) along with a list of column names of the subset.  Let's do that now using the subet **Survived**, **Pclass**, and **Embarked** and store the result in a DataFrame called dfTitanic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfTitanic = dfDirtyT.dropna(subset=[\"Survived\", \"Pclass\", \"Embarked\"]).copy()\n",
    "dfTitanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that that DataFrame has been reduced to 819 rows and that all columns except for **Age** have their full complement of values. \n",
    "\n",
    "We'll come back to the **Age** column in a bit, but for now let's look at how we might deal with the **Fare** column.  Almost anything useful we could do with the **Fare** column is going to require it being formatted as a numerical value.  We can accomplish this by calling .apply( ) on the **Fare** column and passing in a custom made function that unravels the string and replaces it with a float.  We'll write such a function here and explain how it works in the comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fix_fare(val):\n",
    "    \n",
    "    # For safety, if the passed value is missing, don't change it \n",
    "    if pd.isnull(val):\n",
    "        return val \n",
    "    \n",
    "    # With strings, it's a good idea to call strip() to remove\n",
    "    # leading or trailing white space \n",
    "    stripped_val = val.strip()\n",
    "    \n",
    "    # Use string.replace( ) to replace the non numerical characters \n",
    "    # with blanks ('').  Note that '\\u00A3' is the unicode character for\n",
    "    # the English pound sign \n",
    "    num_str = stripped_val.replace('\\u00A3', '').replace('s','')\n",
    "    \n",
    "    # After replacing the errant characters, we still have a string \n",
    "    # We need to convert it to a floating point value \n",
    "    num = float(num_str)\n",
    "    \n",
    "    return num "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll call the .apply( ) function on the **Fare** column and pass in our string-to-number function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfTitanic.loc[:, \"Fare\"] = dfTitanic.loc[:,\"Fare\"].apply(fix_fare)\n",
    "dfTitanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the call to .head( ) it looks as if we successfully transformed the strings into floats, but it's a good idea to call .info( ) again and confirm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfTitanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it looks like we were successful because **Fare** is now of type float64. \n",
    "\n",
    "Note that we kinda did the bare minimum here with the fix_fare function.  We probably should have looked up the fact that there are 20 shillings in a pound and converted the number of shillings into the correct fraction of a pound. But for now, this is sufficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "***\n",
    "\n",
    "Write a function fix_age that replaces string values with numeric values, then replace the **Age** column by calling .apply( ) on it and passing in your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fix_age(val):\n",
    "    \n",
    "    return num \n",
    "\n",
    "dfTitanic.loc[:,\"Age\"] = dfTitanic.loc[:,\"Age\"].apply(fix_age)\n",
    "dfTitanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "***\n",
    "\n",
    "Suppose we'd rather have a column that actually has the port of departure rather than the single-letter code listed in **Embarked**. Write a function initial_to_name to convert **Embarked** initials into the name of the port and create the column **Departure Port** by passing initial_to_name to the .apply( ) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initial_to_name(init):\n",
    "    \n",
    "    return name \n",
    "\n",
    "dfTitanic.loc[:, \"Departure Port\"] = # TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Imputation in Pandas \n",
    "***\n",
    "\n",
    "Data **imputation** is the process of replacing missing values with reasonable guesses based on other information that you know.  A word of warning: **great care** must be taken when undertaking data imputation because replacing missing values with unreasonable things can cause problems in downstream in your pipeline.  \n",
    "\n",
    "We demonstrate the process here for the missing **Age** values just so you can see how the process works. In order to proceed with this example, you first need to make sure that you have successfully completed **Exercise 5**. \n",
    "\n",
    "One way that we might infer the age of a passenger is by looking at the salutation that is given in their name (by _salutation_ we mean things like \"Mr\", \"Mrs\", etc). A cursory look through the **Name** column reveals that the most common salutations are \"Mr\", \"Mrs\", \"Miss\", and \"Master\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Occurances of {}: {}\".format(\"Master\", np.sum(dfTitanic[\"Name\"].str.contains(\"Master\"))))\n",
    "print(\"Occurances of {}: {}\".format(\"Miss\", np.sum(dfTitanic[\"Name\"].str.contains(\"Miss\"))))\n",
    "print(\"Occurances of {}: {}\".format(\"Mrs\", np.sum(dfTitanic[\"Name\"].str.contains(\"Mrs\"))))\n",
    "print(\"Occurances of {}: {}\".format(\"Mr\", np.sum(dfTitanic[\"Name\"].str.contains(\"Mr\")) - np.sum(dfTitanic[\"Name\"].str.contains(\"Mrs\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The str.contains( ) function is very useful for string-based columns.  It returns a boolean Series indicating whether the argument is found in each entry of the column. Note that we had to be a little careful here because any string containing \"Mrs\" will also contain \"Mr\", so we might double count.  It might have been OK to check the salutations with trailing periods (e.g. \"Mr.\" and \"Mrs.\") but we didn't want to bet on the lack of typos in the name entries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a column called **Salutation** to our DataFrame that records the salutation found in the name. If none of the common salutations are present we'll record it as \"Other\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grab_salutation(name):\n",
    "    \n",
    "    # Safely return null values \n",
    "    if pd.isnull(name):\n",
    "        return name \n",
    "    \n",
    "    # Look for occurrances of common Salutations \n",
    "    if (\"Mr\" in name) and (not \"Mrs\" in name):\n",
    "        return \"Mr\"\n",
    "    \n",
    "    if \"Mrs\" in name:\n",
    "        return \"Mrs\"\n",
    "    \n",
    "    if \"Miss\" in name:\n",
    "        return \"Miss\"\n",
    "    \n",
    "    if \"Master\" in name:\n",
    "        return \"Master\"\n",
    "    \n",
    "    return \"Other\"\n",
    "\n",
    "dfTitanic.loc[:, \"Salutation\"] = dfTitanic.loc[:, \"Name\"].apply(grab_salutation)\n",
    "dfTitanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll replace missing **Age** values by the median age of those with the same value of **Salutation**. First we'll show an example of replacing the \"Mr\"s by the median \"Mr\"-age.  Note that passenger with index 5 is a \"Mr\" and is missing the **Age** value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfTitanic.loc[5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To impute missing **Age** values for \"Mr\"s we'll slice into the **Age** column corresponding to rows containing \"Mr\"s and who's **Age** is a null-value.  Then we'll set that slice to the median age of \"Mr\"s. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfTitanic.loc[pd.isnull(dfTitanic[\"Age\"]) & (dfTitanic.loc[:,\"Salutation\"]==\"Mr\"), \"Age\"] = dfTitanic.loc[dfTitanic[\"Salutation\"]==\"Mr\", \"Age\"].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we'll check the status of Mr. James Moran from above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfTitanic.loc[5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Mr. Moran's age has been replaced by the median \"Mr\" age of $30$. \n",
    "\n",
    "To speed things up, we'll do the rest of the imputations in a loop: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for s in [\"Mrs\", \"Miss\", \"Master\"]:\n",
    "    dfTitanic.loc[pd.isnull(dfTitanic[\"Age\"]) & (dfTitanic.loc[:,\"Salutation\"]==s), \"Age\"] = dfTitanic.loc[dfTitanic[\"Salutation\"]==s, \"Age\"].median()\n",
    "    \n",
    "dfTitanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hilariously, it looks like there was only one row with a missing age where the passengers salutation was \"Other\".  We've successfully imputed all but one of the missing age values. We'll happily drop this offending row from the DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfTitanic = dfTitanic.dropna(subset=[\"Age\"])\n",
    "dfTitanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it!  A completely clean data set with 818 fully outfitted rows, which is pretty good because we started with a messy data set with 891 rows.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
